automl_classifier.py

import pandas as pd
from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("B1_toyato_feature.csv")

# Convert 'Price' into a classification target (example: high vs low price)
median_price = df["Price"].median()
df["Price_Class"] = df["Price"].apply(lambda x: 1 if x >= median_price else 0)

# Prepare data
X = df.drop(columns=["Price", "Price_Class"])
y = df["Price_Class"]

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# TPOT Classifier
tpot = TPOTClassifier(
    generations=5,
    population_size=20,
    verbosity=2,
    random_state=42,
    n_jobs=-1,
    cv=5
)

# Train model
tpot.fit(X_train, y_train)

# Evaluate
accuracy = tpot.score(X_test, y_test)
print(f"Test Set Accuracy: {accuracy:.2f}")

# Export best model
tpot.export("best_classifier_pipeline.py")


#docker build -t automl-tpot .
#docker run --rm -v %cd%:/app automl-tpot python automl_script.py
#docker run --rm -v ${PWD}:/app automl-tpot python automl_script.py

best_classifier_pipeline

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=42)

imputer = SimpleImputer(strategy="median")
imputer.fit(training_features)
training_features = imputer.transform(training_features)
testing_features = imputer.transform(testing_features)

# Average CV score on the training set was: 1.0
exported_pipeline = RandomForestClassifier(bootstrap=False, criterion="entropy", max_features=0.4, min_samples_leaf=10, min_samples_split=17, n_estimators=100)
# Fix random state in exported estimator
if hasattr(exported_pipeline, 'random_state'):
    setattr(exported_pipeline, 'random_state', 42)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)

Dockerfile

# Use an official Python runtime as a parent image
FROM python:3.8-slim

# Set the working directory in the container
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Make port 80 available to the world outside the container
EXPOSE 80

# Define environment variable
ENV PYTHONUNBUFFERED 1

# Run the script when the container launches
CMD ["python", "automl_model.py"]

pandas
numpy
scikit-learn
tpot==0.12.1
xgboost==1.7.6
xgboost==1.7.6 ; platform_system != "Linux" or platform_machine != "x86_64"